# def explain_prediction(probability, input_dict, surname):

#     prompt = f"""You are an expert data scientist at a bank, where you specialise in interpreting and explaining the predictions of machine learning models.

#     your machine learning model has predicted that a customer named {surname} has a {round(probability*100, 1)}% probability of churning, based on the information provided below.
#         Here is the customer's information:
#         {input_dict}

#         Here are the machine learning model's top most important features for predicting churn:

#         Feature | Importance
#         ______________________
#         NumOfProducts | 0.323888
#         IsActiveMember | 0.164146
#         Age | 0.109550
#         Geography_Germany | 0.091373
#         Balance | 0.052786
#         Geography_France | 0.046463
#         Gender_Female | 0.045283
#         Geography_Spain | 0.036855
#         CreditScore | 0.035005
#         EstimatedSalary | 0.032655
#         HasCrCard | 0.031940
#         Tenure | 0.030054
#         Gender_Male | 0.000000

#     {pd.set_option('display.max_columns', None)}

#     Here are summary statistics for the churned customers:
#     {df[df['Exited'] == 1].describe()}

# - If the customer has over a 40% risk of churning, generate a 3 sentence explanation of why they are at risk of churning.
# - If the customer has less than a 40% risk of churning, generate a 3 sentence explanation of why they might not be at risk of churning.
# - Your explanation should be based on the customer's information, the summary statistics of churned and non-churned customers, and the feature importances provided.

#     Don't mention the probablity of churning, or the machine learning model, or say anything like "Based on the machine learning model's prediction..." or "The model predicts that...". Instead, write as if you are an expert data scientist at a bank, and you are explaining the prediction to a non-technical audience.
#     """

#     print("Explanation Prompt: ", prompt)


#     raw_response = client.chat.completions.create(
#         model="llama-3.2-3b-preview",
#         messages=[{"role": "user", "content": prompt}],
#     )
#     return raw_response.choices[0].message.content
# Update the explain_prediction function to use the new client